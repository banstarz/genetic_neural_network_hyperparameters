{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 15,
>>>>>>> 94153503a237733fffb1abb80ba5885666bc5bbf
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4430,
     "status": "ok",
     "timestamp": 1582952405639,
     "user": {
      "displayName": "Ренат Карымов",
      "photoUrl": "https://lh3.googleusercontent.com/-yQbebsvB8Ys/AAAAAAAAAAI/AAAAAAAAAC0/dANQJOWntL4/s64/photo.jpg",
      "userId": "03898504137032145509"
     },
     "user_tz": -420
    },
    "id": "mwStXzMwzun4",
<<<<<<< HEAD
    "outputId": "3fc80251-24ae-4074-95ed-aeafb05ab385"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import random\n",
=======
    "outputId": "3fc80251-24ae-4074-95ed-aeafb05ab385",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
>>>>>>> 94153503a237733fffb1abb80ba5885666bc5bbf
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
<<<<<<< HEAD
=======
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
>>>>>>> 94153503a237733fffb1abb80ba5885666bc5bbf
    "class_mapping = {label:idx for idx,label in\n",
    "               enumerate(np.unique(data[ 'diagnosis' ] )) }\n",
    "x = data.iloc[ : , 2: ]  \n",
    "y = data.iloc [ : , 1].values\n",
    "for i in range(len(y)):\n",
    "    y[i] = class_mapping[y[i]]\n",
<<<<<<< HEAD
    "x_train, x_test, y_train, y_test = train_test_split(x, y , test_size=0.1, random_state=0)\n",
    "\n",
    "x_train = np.asarray(x_train).astype(np.float32)\n",
    "x_test = np.asarray(x_test).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "y_test = np.asarray(y_test).astype(np.float32)"
=======
    "\n",
    "x.head()"
>>>>>>> 94153503a237733fffb1abb80ba5885666bc5bbf
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SBRtuyNLvgT9"
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, BatchSize, ActFunc, Optimizer, Loss, NeuronList):\n",
    "        self.ActFunc = ActFunc\n",
    "        self.Optimizer = Optimizer\n",
    "        self.BatchSize = BatchSize\n",
    "        self.LayerNumber = 2\n",
    "        self.NeuronList = NeuronList\n",
    "        self.CrossValidCount = 3\n",
    "        self.Loss = Loss\n",
    "        self.Chromosome = [BatchSize, ActFunc, Optimizer, Loss, NeuronList]\n",
    "\n",
    "    def ModelFit(self, train_data, train_labels):\n",
    "        network = models.Sequential()\n",
    "        network.add(layers.Dense(self.NeuronList[0], activation=self.ActFunc, input_shape=(30, )))\n",
    "        for i in range(1, self.LayerNumber):\n",
    "            network.add(layers.Dense(self.NeuronList[i], activation=self.ActFunc))\n",
    "        network.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        network.compile(optimizer=self.Optimizer, loss = self.Loss,\n",
    "                                            metrics=['acc'])\n",
    "        self.history = [0 for i in range(self.CrossValidCount)]\n",
    "        for i in range(self.CrossValidCount):\n",
    "            self.history[i] = network.fit(train_data, train_labels, epochs=50, batch_size=self.BatchSize, verbose=1, validation_split = 0.3)\n",
    "        self.EvalAcc()\n",
    "        \n",
    "    def EvalAcc(self):\n",
    "        self.Acc = 0\n",
    "        for i in range(self.CrossValidCount):\n",
    "            self.Acc += max(self.history[i].history['val_acc'])\n",
    "        self.Acc /= self.CrossValidCount\n",
    "    \n",
    "    def GetAcc(self):\n",
    "        return self.Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
=======
   "execution_count": 17,
   "metadata": {},
>>>>>>> 94153503a237733fffb1abb80ba5885666bc5bbf
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch 1/50\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 0.2691 - acc: 0.5000 - val_loss: 0.2287 - val_acc: 0.8117\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2152 - acc: 0.6899 - val_loss: 0.2117 - val_acc: 0.6364\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2011 - acc: 0.6816 - val_loss: 0.1981 - val_acc: 0.6818\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1837 - acc: 0.7570 - val_loss: 0.1830 - val_acc: 0.7273\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1631 - acc: 0.8352 - val_loss: 0.1629 - val_acc: 0.8182\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1466 - acc: 0.8827 - val_loss: 0.1484 - val_acc: 0.8377\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1308 - acc: 0.9050 - val_loss: 0.1387 - val_acc: 0.8831\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1229 - acc: 0.8966 - val_loss: 0.1255 - val_acc: 0.8571\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1097 - acc: 0.8911 - val_loss: 0.1136 - val_acc: 0.8571\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0969 - acc: 0.9050 - val_loss: 0.1110 - val_acc: 0.8571\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0925 - acc: 0.9022 - val_loss: 0.1034 - val_acc: 0.8766\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0851 - acc: 0.9106 - val_loss: 0.0991 - val_acc: 0.8831\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0798 - acc: 0.9106 - val_loss: 0.0957 - val_acc: 0.8896\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0782 - acc: 0.9162 - val_loss: 0.0900 - val_acc: 0.8896\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0749 - acc: 0.9190 - val_loss: 0.0892 - val_acc: 0.9026\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0731 - acc: 0.9162 - val_loss: 0.0885 - val_acc: 0.8896\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0705 - acc: 0.9134 - val_loss: 0.0868 - val_acc: 0.8896\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0727 - acc: 0.9050 - val_loss: 0.0876 - val_acc: 0.9026\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0678 - acc: 0.9218 - val_loss: 0.0778 - val_acc: 0.8831\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0707 - acc: 0.9106 - val_loss: 0.0816 - val_acc: 0.8896\n",
      "Epoch 21/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0658 - acc: 0.9134 - val_loss: 0.0790 - val_acc: 0.8831\n",
      "Epoch 22/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0653 - acc: 0.9246 - val_loss: 0.0852 - val_acc: 0.8961\n",
      "Epoch 23/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0677 - acc: 0.9162 - val_loss: 0.0735 - val_acc: 0.9091\n",
      "Epoch 24/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0665 - acc: 0.9078 - val_loss: 0.0821 - val_acc: 0.8896\n",
      "Epoch 25/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0635 - acc: 0.9218 - val_loss: 0.0770 - val_acc: 0.9026\n",
      "Epoch 26/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0686 - acc: 0.9050 - val_loss: 0.0810 - val_acc: 0.8961\n",
      "Epoch 27/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0705 - acc: 0.9050 - val_loss: 0.0778 - val_acc: 0.9026\n",
      "Epoch 28/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0679 - acc: 0.8994 - val_loss: 0.0757 - val_acc: 0.9026\n",
      "Epoch 29/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0621 - acc: 0.9218 - val_loss: 0.0904 - val_acc: 0.8896\n",
      "Epoch 30/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0667 - acc: 0.9106 - val_loss: 0.0726 - val_acc: 0.9091\n",
      "Epoch 31/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0677 - acc: 0.9134 - val_loss: 0.0729 - val_acc: 0.8896\n",
      "Epoch 32/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0646 - acc: 0.9162 - val_loss: 0.0776 - val_acc: 0.8961\n",
      "Epoch 33/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0644 - acc: 0.9190 - val_loss: 0.0734 - val_acc: 0.9026\n",
      "Epoch 34/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0613 - acc: 0.9106 - val_loss: 0.0769 - val_acc: 0.8961\n",
      "Epoch 35/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0638 - acc: 0.9078 - val_loss: 0.0798 - val_acc: 0.8831\n",
      "Epoch 36/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0620 - acc: 0.9106 - val_loss: 0.0770 - val_acc: 0.8961\n",
      "Epoch 37/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0583 - acc: 0.9190 - val_loss: 0.0855 - val_acc: 0.8961\n",
      "Epoch 38/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0608 - acc: 0.9162 - val_loss: 0.0747 - val_acc: 0.8896\n",
      "Epoch 39/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0626 - acc: 0.9134 - val_loss: 0.0697 - val_acc: 0.8896\n",
      "Epoch 40/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0579 - acc: 0.9246 - val_loss: 0.0703 - val_acc: 0.8896\n",
      "Epoch 41/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0629 - acc: 0.9134 - val_loss: 0.0673 - val_acc: 0.9026\n",
      "Epoch 42/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0603 - acc: 0.9106 - val_loss: 0.0662 - val_acc: 0.9091\n",
      "Epoch 43/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0616 - acc: 0.9218 - val_loss: 0.0737 - val_acc: 0.8961\n",
      "Epoch 44/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0615 - acc: 0.9162 - val_loss: 0.0783 - val_acc: 0.9026\n",
      "Epoch 45/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0614 - acc: 0.9050 - val_loss: 0.0803 - val_acc: 0.8896\n",
      "Epoch 46/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0589 - acc: 0.9050 - val_loss: 0.0757 - val_acc: 0.9091\n",
      "Epoch 47/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0638 - acc: 0.9190 - val_loss: 0.0710 - val_acc: 0.9091\n",
      "Epoch 48/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0530 - acc: 0.9190 - val_loss: 0.0714 - val_acc: 0.9026\n",
      "Epoch 49/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0534 - acc: 0.9246 - val_loss: 0.0728 - val_acc: 0.8961\n",
      "Epoch 50/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0574 - acc: 0.9246 - val_loss: 0.0718 - val_acc: 0.9091\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0581 - acc: 0.9246 - val_loss: 0.0634 - val_acc: 0.9221\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0582 - acc: 0.9302 - val_loss: 0.0664 - val_acc: 0.9091\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0576 - acc: 0.9302 - val_loss: 0.0721 - val_acc: 0.8896\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0591 - acc: 0.9218 - val_loss: 0.0684 - val_acc: 0.9026\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0570 - acc: 0.9162 - val_loss: 0.0623 - val_acc: 0.9026\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0592 - acc: 0.9190 - val_loss: 0.0647 - val_acc: 0.9091\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0536 - acc: 0.9330 - val_loss: 0.0668 - val_acc: 0.9026\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0536 - acc: 0.9246 - val_loss: 0.0653 - val_acc: 0.9286\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0525 - acc: 0.9330 - val_loss: 0.0701 - val_acc: 0.9221\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0587 - acc: 0.9274 - val_loss: 0.0714 - val_acc: 0.9156\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0539 - acc: 0.9330 - val_loss: 0.0719 - val_acc: 0.9156\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0568 - acc: 0.9246 - val_loss: 0.0646 - val_acc: 0.9091\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0503 - acc: 0.9385 - val_loss: 0.0798 - val_acc: 0.9026\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0598 - acc: 0.9190 - val_loss: 0.0669 - val_acc: 0.9156\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0546 - acc: 0.9246 - val_loss: 0.0692 - val_acc: 0.9091\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0539 - acc: 0.9302 - val_loss: 0.0774 - val_acc: 0.9026\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0597 - acc: 0.9134 - val_loss: 0.0657 - val_acc: 0.9091\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0548 - acc: 0.9162 - val_loss: 0.0674 - val_acc: 0.8831\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0600 - acc: 0.9106 - val_loss: 0.0717 - val_acc: 0.9221\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0624 - acc: 0.9050 - val_loss: 0.0580 - val_acc: 0.9221\n",
      "Epoch 21/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0567 - acc: 0.9162 - val_loss: 0.0844 - val_acc: 0.8961\n",
      "Epoch 22/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0604 - acc: 0.9162 - val_loss: 0.0681 - val_acc: 0.9156\n",
      "Epoch 23/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0527 - acc: 0.9302 - val_loss: 0.0694 - val_acc: 0.9026\n",
      "Epoch 24/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0542 - acc: 0.9274 - val_loss: 0.0600 - val_acc: 0.9026\n",
      "Epoch 25/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0582 - acc: 0.9218 - val_loss: 0.0607 - val_acc: 0.9026\n",
      "Epoch 26/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0512 - acc: 0.9358 - val_loss: 0.0601 - val_acc: 0.9026\n",
      "Epoch 27/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0512 - acc: 0.9274 - val_loss: 0.0593 - val_acc: 0.9221\n",
      "Epoch 28/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0592 - acc: 0.9078 - val_loss: 0.0606 - val_acc: 0.9221\n",
      "Epoch 29/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0542 - acc: 0.9330 - val_loss: 0.0643 - val_acc: 0.9091\n",
      "Epoch 30/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0521 - acc: 0.9274 - val_loss: 0.0585 - val_acc: 0.9156\n",
      "Epoch 31/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0536 - acc: 0.9218 - val_loss: 0.0577 - val_acc: 0.9156\n",
      "Epoch 32/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0492 - acc: 0.9330 - val_loss: 0.0573 - val_acc: 0.9156\n",
      "Epoch 33/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0534 - acc: 0.9218 - val_loss: 0.0524 - val_acc: 0.9351\n",
      "Epoch 34/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0511 - acc: 0.9385 - val_loss: 0.0593 - val_acc: 0.9091\n",
      "Epoch 35/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0607 - acc: 0.9218 - val_loss: 0.0555 - val_acc: 0.9091\n",
      "Epoch 36/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0501 - acc: 0.9330 - val_loss: 0.0658 - val_acc: 0.9351\n",
      "Epoch 37/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0538 - acc: 0.9274 - val_loss: 0.0513 - val_acc: 0.9351\n",
      "Epoch 38/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0529 - acc: 0.9302 - val_loss: 0.0648 - val_acc: 0.9286\n",
      "Epoch 39/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0510 - acc: 0.9246 - val_loss: 0.0588 - val_acc: 0.9026\n",
      "Epoch 40/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0529 - acc: 0.9302 - val_loss: 0.0663 - val_acc: 0.9091\n",
      "Epoch 41/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0506 - acc: 0.9330 - val_loss: 0.0638 - val_acc: 0.9091\n",
      "Epoch 42/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0548 - acc: 0.9330 - val_loss: 0.0626 - val_acc: 0.9221\n",
      "Epoch 43/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0522 - acc: 0.9330 - val_loss: 0.0666 - val_acc: 0.9026\n",
      "Epoch 44/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0550 - acc: 0.9246 - val_loss: 0.0588 - val_acc: 0.9221\n",
      "Epoch 45/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0487 - acc: 0.9385 - val_loss: 0.0568 - val_acc: 0.9026\n",
      "Epoch 46/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0513 - acc: 0.9246 - val_loss: 0.0584 - val_acc: 0.9091\n",
      "Epoch 47/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0523 - acc: 0.9358 - val_loss: 0.0663 - val_acc: 0.9026\n",
      "Epoch 48/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0545 - acc: 0.9302 - val_loss: 0.0569 - val_acc: 0.9156\n",
      "Epoch 49/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0517 - acc: 0.9246 - val_loss: 0.0643 - val_acc: 0.9026\n",
      "Epoch 50/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0512 - acc: 0.9330 - val_loss: 0.0679 - val_acc: 0.9221\n",
      "Epoch 1/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0500 - acc: 0.9246 - val_loss: 0.0553 - val_acc: 0.9221\n",
      "Epoch 2/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0480 - acc: 0.9385 - val_loss: 0.0711 - val_acc: 0.9091\n",
      "Epoch 3/50\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.0225 - acc: 1.000 - 0s 2ms/step - loss: 0.0511 - acc: 0.9358 - val_loss: 0.0528 - val_acc: 0.9351\n",
      "Epoch 4/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0481 - acc: 0.9358 - val_loss: 0.0566 - val_acc: 0.9156\n",
      "Epoch 5/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0517 - acc: 0.9274 - val_loss: 0.0494 - val_acc: 0.9156\n",
      "Epoch 6/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0492 - acc: 0.9246 - val_loss: 0.0574 - val_acc: 0.9091\n",
      "Epoch 7/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0457 - acc: 0.9385 - val_loss: 0.0655 - val_acc: 0.9286\n",
      "Epoch 8/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0483 - acc: 0.9413 - val_loss: 0.0657 - val_acc: 0.9026\n",
      "Epoch 9/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0501 - acc: 0.9358 - val_loss: 0.0554 - val_acc: 0.9286\n",
      "Epoch 10/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0433 - acc: 0.9413 - val_loss: 0.0563 - val_acc: 0.9221\n",
      "Epoch 11/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0450 - acc: 0.9274 - val_loss: 0.0495 - val_acc: 0.9156\n",
      "Epoch 12/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0529 - acc: 0.9302 - val_loss: 0.0514 - val_acc: 0.9156\n",
      "Epoch 13/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0450 - acc: 0.9525 - val_loss: 0.0533 - val_acc: 0.9351\n",
      "Epoch 14/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0498 - acc: 0.9358 - val_loss: 0.0524 - val_acc: 0.9351\n",
      "Epoch 15/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0485 - acc: 0.9330 - val_loss: 0.0677 - val_acc: 0.9221\n",
      "Epoch 16/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0513 - acc: 0.9385 - val_loss: 0.0486 - val_acc: 0.9286\n",
      "Epoch 17/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0475 - acc: 0.9302 - val_loss: 0.0572 - val_acc: 0.9351\n",
      "Epoch 18/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0442 - acc: 0.9469 - val_loss: 0.0712 - val_acc: 0.9091\n",
      "Epoch 19/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0422 - acc: 0.9441 - val_loss: 0.0566 - val_acc: 0.9351\n",
      "Epoch 20/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0496 - acc: 0.9358 - val_loss: 0.0601 - val_acc: 0.9026\n",
      "Epoch 21/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0443 - acc: 0.9441 - val_loss: 0.0523 - val_acc: 0.9351\n",
      "Epoch 22/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0508 - acc: 0.9358 - val_loss: 0.0593 - val_acc: 0.9221\n",
      "Epoch 23/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0457 - acc: 0.9497 - val_loss: 0.0679 - val_acc: 0.9221\n",
      "Epoch 24/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0427 - acc: 0.9497 - val_loss: 0.0536 - val_acc: 0.9351\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0482 - acc: 0.9302 - val_loss: 0.0505 - val_acc: 0.9351\n",
      "Epoch 26/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0422 - acc: 0.9469 - val_loss: 0.0462 - val_acc: 0.9221\n",
      "Epoch 27/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0455 - acc: 0.9358 - val_loss: 0.0564 - val_acc: 0.9156\n",
      "Epoch 28/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0450 - acc: 0.9385 - val_loss: 0.0542 - val_acc: 0.9156\n",
      "Epoch 29/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0562 - acc: 0.9274 - val_loss: 0.0487 - val_acc: 0.9286\n",
      "Epoch 30/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0473 - acc: 0.9385 - val_loss: 0.0634 - val_acc: 0.9156\n",
      "Epoch 31/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0498 - acc: 0.9413 - val_loss: 0.0450 - val_acc: 0.9286\n",
      "Epoch 32/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0451 - acc: 0.9358 - val_loss: 0.0494 - val_acc: 0.9286\n",
      "Epoch 33/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0396 - acc: 0.9413 - val_loss: 0.0835 - val_acc: 0.8961\n",
      "Epoch 34/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0523 - acc: 0.9330 - val_loss: 0.0483 - val_acc: 0.9351\n",
      "Epoch 35/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0505 - acc: 0.9330 - val_loss: 0.0441 - val_acc: 0.9351\n",
      "Epoch 36/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0466 - acc: 0.9441 - val_loss: 0.0427 - val_acc: 0.9481\n",
      "Epoch 37/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0455 - acc: 0.9385 - val_loss: 0.0417 - val_acc: 0.9351\n",
      "Epoch 38/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0535 - acc: 0.9274 - val_loss: 0.0579 - val_acc: 0.9286\n",
      "Epoch 39/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0484 - acc: 0.9358 - val_loss: 0.0564 - val_acc: 0.9286\n",
      "Epoch 40/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0439 - acc: 0.9497 - val_loss: 0.0609 - val_acc: 0.9221\n",
      "Epoch 41/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0511 - acc: 0.9330 - val_loss: 0.0435 - val_acc: 0.9416\n",
      "Epoch 42/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0511 - acc: 0.9358 - val_loss: 0.0409 - val_acc: 0.9416\n",
      "Epoch 43/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0471 - acc: 0.9413 - val_loss: 0.0417 - val_acc: 0.9351\n",
      "Epoch 44/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0441 - acc: 0.9497 - val_loss: 0.0479 - val_acc: 0.9221\n",
      "Epoch 45/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0419 - acc: 0.9497 - val_loss: 0.0533 - val_acc: 0.9286\n",
      "Epoch 46/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0462 - acc: 0.9469 - val_loss: 0.0409 - val_acc: 0.9481\n",
      "Epoch 47/50\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0407 - acc: 0.9385 - val_loss: 0.0640 - val_acc: 0.9156\n",
      "Epoch 48/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0426 - acc: 0.9469 - val_loss: 0.0578 - val_acc: 0.9221\n",
      "Epoch 49/50\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0444 - acc: 0.9469 - val_loss: 0.0514 - val_acc: 0.9351\n",
      "Epoch 50/50\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.0015 - acc: 1.000 - 0s 2ms/step - loss: 0.0512 - acc: 0.9358 - val_loss: 0.0517 - val_acc: 0.9416\n"
=======
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1\n",
      " 0 1 0 1 1 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0\n",
      " 0 1 0 0 0 0 0 1 1 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0\n",
      " 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1\n",
      " 1 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 0 0\n",
      " 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0]\n"
>>>>>>> 94153503a237733fffb1abb80ba5885666bc5bbf
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "Network = NN(BatchSize=10, ActFunc='sigmoid', Optimizer='rmsprop', Loss='MSE', NeuronList=[20, 20])\n",
    "Network.ModelFit(x_train, y_train)"
=======
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y , test_size=0.1, random_state=0)\n",
    "\n",
    "x_train = np.asarray(x_train).astype(np.float32)\n",
    "x_test = np.asarray(x_test).astype(np.float32)\n",
    "y_train = np.asarray(y_train).astype(np.float32)\n",
    "y_test = np.asarray(y_test).astype(np.float32)"
>>>>>>> 94153503a237733fffb1abb80ba5885666bc5bbf
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GA&NN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
